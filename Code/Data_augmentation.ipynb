{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/gaurav/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gaurav/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from random import randint\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(sentence, n):\n",
    "    old_sentence = sentence\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in nltk.corpus.stopwords.words('english')]))\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = synonyms[0]\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n: # n is the number of words to be replaced\n",
    "            break\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    if old_sentence == sentence:\n",
    "        flag = False\n",
    "    else:\n",
    "        flag = True\n",
    "    return flag, sentence\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char.isalpha() or char == ' '])\n",
    "            synonyms.add(synonym)\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preReviewsCSV.csv')\n",
    "df.dropna(subset=['reviewContent', 'label'], inplace=True)\n",
    "df1 = df[df.label == 1]\n",
    "# df2 = df[df.label == 0].sample(n=df1.shape[0], random_state=1)\n",
    "# new_df = pd.concat([df1, df2], ignore_index=True)\n",
    "# data = new_df.sample(frac=1, random_state=2)\n",
    "\n",
    "# train_data = data.sample(frac=0.8, random_state=1)\n",
    "# val_data = data.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46747/66890\r"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "fake_df = pd.DataFrame(columns=['date', 'reviewID', 'reviewerID', 'reviewContent', 'rating',\n",
    "       'usefulCount', 'coolCount', 'funnyCount', 'flagged', 'restaurantID',\n",
    "       'label', 'doc_embeddings', 'Polarity'])\n",
    "percentages = [0.1, 0.2, 0.3, 0.5, 0.7, 0.9]\n",
    "for i, row in df1.iterrows():\n",
    "    sent = row['reviewContent']\n",
    "    new_sents = []\n",
    "    for p in percentages:\n",
    "        flag, new_sent = synonym_replacement(sent, int(p*len(sent.split(' '))))\n",
    "        if flag:\n",
    "            cnt += 1\n",
    "            new_sents.append(new_sent)\n",
    "    new_sents = list(set(new_sents)) + [sent]\n",
    "\n",
    "    for s in new_sents:\n",
    "        new_row = {'date': row['date'], 'reviewID': row['reviewID'], 'reviewerID': row['reviewerID'], 'reviewContent': s, 'rating': row['rating'], \n",
    "                   'usefulCount': row['usefulCount'], 'coolCount': row['coolCount'], 'funnyCount': row['funnyCount'], 'flagged': row['flagged'], \n",
    "                   'restaurantID': row['restaurantID'], 'label': row['label'], 'doc_embeddings': row['doc_embeddings'], 'Polarity': row['Polarity']}\n",
    "        fake_df = pd.concat([fake_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            \n",
    "    print(f\"{len(fake_df)}/{i+1}\", end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>reviewID</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewContent</th>\n",
       "      <th>rating</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>coolCount</th>\n",
       "      <th>funnyCount</th>\n",
       "      <th>flagged</th>\n",
       "      <th>restaurantID</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_embeddings</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8/25/2012</td>\n",
       "      <td>XgqV3Ac0CLg2R-_c-yZvTw</td>\n",
       "      <td>tfn2U8XfLaJnRD9EXpTxLQ</td>\n",
       "      <td>food for thought amaz two ticket tomorrow dark...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>pbEiXam9YJL3neCYHGwLUA</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.01494107  0.79770064  0.57676816  0.156622...</td>\n",
       "      <td>0.7184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8/25/2012</td>\n",
       "      <td>XgqV3Ac0CLg2R-_c-yZvTw</td>\n",
       "      <td>tfn2U8XfLaJnRD9EXpTxLQ</td>\n",
       "      <td>food for thought amaz  slate tomorrow dark bil...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>pbEiXam9YJL3neCYHGwLUA</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.01494107  0.79770064  0.57676816  0.156622...</td>\n",
       "      <td>0.7184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8/25/2012</td>\n",
       "      <td>XgqV3Ac0CLg2R-_c-yZvTw</td>\n",
       "      <td>tfn2U8XfLaJnRD9EXpTxLQ</td>\n",
       "      <td>food amaz two ticket tomorrow night sunday any...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>pbEiXam9YJL3neCYHGwLUA</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.01494107  0.79770064  0.57676816  0.156622...</td>\n",
       "      <td>0.7184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8/25/2012</td>\n",
       "      <td>XgqV3Ac0CLg2R-_c-yZvTw</td>\n",
       "      <td>tfn2U8XfLaJnRD9EXpTxLQ</td>\n",
       "      <td>food amaz two ticket tomorrow dark sunday anyo...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>pbEiXam9YJL3neCYHGwLUA</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.01494107  0.79770064  0.57676816  0.156622...</td>\n",
       "      <td>0.7184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8/25/2012</td>\n",
       "      <td>XgqV3Ac0CLg2R-_c-yZvTw</td>\n",
       "      <td>tfn2U8XfLaJnRD9EXpTxLQ</td>\n",
       "      <td>food amaz two ticket tomorrow dark sunday anyo...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>pbEiXam9YJL3neCYHGwLUA</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.01494107  0.79770064  0.57676816  0.156622...</td>\n",
       "      <td>0.7184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                reviewID              reviewerID  \\\n",
       "0  8/25/2012  XgqV3Ac0CLg2R-_c-yZvTw  tfn2U8XfLaJnRD9EXpTxLQ   \n",
       "1  8/25/2012  XgqV3Ac0CLg2R-_c-yZvTw  tfn2U8XfLaJnRD9EXpTxLQ   \n",
       "2  8/25/2012  XgqV3Ac0CLg2R-_c-yZvTw  tfn2U8XfLaJnRD9EXpTxLQ   \n",
       "3  8/25/2012  XgqV3Ac0CLg2R-_c-yZvTw  tfn2U8XfLaJnRD9EXpTxLQ   \n",
       "4  8/25/2012  XgqV3Ac0CLg2R-_c-yZvTw  tfn2U8XfLaJnRD9EXpTxLQ   \n",
       "\n",
       "                                       reviewContent rating usefulCount  \\\n",
       "0  food for thought amaz two ticket tomorrow dark...      5           0   \n",
       "1  food for thought amaz  slate tomorrow dark bil...      5           0   \n",
       "2  food amaz two ticket tomorrow night sunday any...      5           0   \n",
       "3  food amaz two ticket tomorrow dark sunday anyo...      5           0   \n",
       "4  food amaz two ticket tomorrow dark sunday anyo...      5           0   \n",
       "\n",
       "  coolCount funnyCount flagged            restaurantID label  \\\n",
       "0         0          0       Y  pbEiXam9YJL3neCYHGwLUA     1   \n",
       "1         0          0       Y  pbEiXam9YJL3neCYHGwLUA     1   \n",
       "2         0          0       Y  pbEiXam9YJL3neCYHGwLUA     1   \n",
       "3         0          0       Y  pbEiXam9YJL3neCYHGwLUA     1   \n",
       "4         0          0       Y  pbEiXam9YJL3neCYHGwLUA     1   \n",
       "\n",
       "                                      doc_embeddings  Polarity  \n",
       "0  [-0.01494107  0.79770064  0.57676816  0.156622...    0.7184  \n",
       "1  [-0.01494107  0.79770064  0.57676816  0.156622...    0.7184  \n",
       "2  [-0.01494107  0.79770064  0.57676816  0.156622...    0.7184  \n",
       "3  [-0.01494107  0.79770064  0.57676816  0.156622...    0.7184  \n",
       "4  [-0.01494107  0.79770064  0.57676816  0.156622...    0.7184  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46747, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df.label == 0].sample(n=fake_df.shape[0], random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([df2, fake_df], ignore_index=True).sample(frac=1, random_state=123)\n",
    "new_df.to_csv('augmentedReviewsCSV.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding userCentric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('augmentedReviewsCSV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'reviewID', 'reviewerID', 'reviewContent', 'rating',\n",
       "       'usefulCount', 'coolCount', 'funnyCount', 'flagged', 'restaurantID',\n",
       "       'label', 'doc_embeddings', 'Polarity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average review length per reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_length'] = df['reviewContent'].str.len()\n",
    "avg_review_length_per_user = df.groupby('reviewerID')['review_length'].mean().reset_index(name='avg_review_length')\n",
    "df = pd.merge(df, avg_review_length_per_user, on='reviewerID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'reviewID', 'reviewerID', 'reviewContent', 'rating',\n",
       "       'usefulCount', 'coolCount', 'funnyCount', 'flagged', 'restaurantID',\n",
       "       'label', 'doc_embeddings', 'Polarity', 'review_length',\n",
       "       'avg_review_length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Std deviation of ratings of the reviewer's reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev_ratings_per_user = df.groupby('reviewerID')['rating'].std().reset_index(name='std_dev_ratings')\n",
    "df = pd.merge(df, std_dev_ratings_per_user, on='reviewerID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'reviewID', 'reviewerID', 'reviewContent', 'rating',\n",
       "       'usefulCount', 'coolCount', 'funnyCount', 'flagged', 'restaurantID',\n",
       "       'label', 'doc_embeddings', 'Polarity', 'review_length',\n",
       "       'avg_review_length', 'std_dev_ratings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Max no. of reviews in a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], format = 'mixed', errors = 'coerce')\n",
    "reviews_per_user_day = df.groupby(['reviewerID', df['date'].dt.date]).size().reset_index(name='daily_reviews')\n",
    "max_reviews_per_user = reviews_per_user_day.groupby('reviewerID')['daily_reviews'].max().reset_index(name='max_daily_reviews')\n",
    "df = pd.merge(df, max_reviews_per_user, on='reviewerID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_feat_included_df = df[['reviewContent', 'avg_review_length', 'std_dev_ratings', 'max_daily_reviews', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93494, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_feat_included_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_feat_included_df.to_csv('newCSV.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
